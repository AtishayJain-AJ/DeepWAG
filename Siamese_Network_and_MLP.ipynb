{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese Network and MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "147KDtieTh41raG3WUTrMmNr4dxT5gqCD",
      "authorship_tag": "ABX9TyOSrT921IKqK9XqXdp9Rirc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtishayJain-AJ/DeepWAG/blob/master/Siamese_Network_and_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkBof4wsdnoO"
      },
      "source": [
        "# This code has been modified from the original source code which can be found at - https://github.com/adambielski/siamese-triplet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xucf0zVVZ48A"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14zIyKMMrSzp"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet loss\n",
        "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative, size_average=True):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1).pow(.5)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1).pow(.5)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        return losses.mean() if size_average else losses.sum()\n",
        "\n",
        "\n",
        "\n",
        "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n",
        "        start_epoch=0):\n",
        "    \"\"\"\n",
        "    Loaders, model, loss function and metrics should work together for a given task,\n",
        "    i.e. The model should be able to process data output of loaders,\n",
        "    loss function should process target output of loaders and outputs from the model\n",
        "    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n",
        "    Siamese network: Siamese loader, siamese model, contrastive loss\n",
        "    Online triplet learning: batch loader, embedding model, online triplet loss\n",
        "    \"\"\"\n",
        "    for epoch in range(0, start_epoch):\n",
        "        scheduler.step()\n",
        "\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        scheduler.step()\n",
        "\n",
        "        # Train stage\n",
        "        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n",
        "\n",
        "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
        "        for metric in metrics:\n",
        "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
        "\n",
        "        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
        "                                                                                 val_loss)\n",
        "        for metric in metrics:\n",
        "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
        "\n",
        "        print(message)\n",
        "\n",
        "\n",
        "def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n",
        "    for metric in metrics:\n",
        "        metric.reset()\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        target = target.long() if len(target) > 0 else None\n",
        "        if not type(data) in (tuple, list):\n",
        "            data = (data,)\n",
        "        if cuda:\n",
        "            data = tuple(d.cuda() for d in data)\n",
        "            if target is not None:\n",
        "                target = target.cuda()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(*data)\n",
        "\n",
        "\n",
        "        if type(outputs) not in (tuple, list):\n",
        "            outputs = (outputs,)\n",
        "\n",
        "        loss_inputs = outputs\n",
        "        if target is not None:\n",
        "            target = (target,)\n",
        "            loss_inputs += target\n",
        "\n",
        "\n",
        "        loss_outputs = loss_fn(*loss_inputs)\n",
        "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
        "        regularization_loss = 0\n",
        "        for param in model.parameters():\n",
        "            regularization_loss = torch.sum(torch.abs(param))\n",
        "            break\n",
        "\n",
        "        loss = loss + 0.00001 * regularization_loss\n",
        "        losses.append(loss.item())\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        for metric in metrics:\n",
        "            metric(outputs, target, loss_outputs)\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                batch_idx * len(data[0]), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
        "            for metric in metrics:\n",
        "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
        "\n",
        "            print(message)\n",
        "            losses = []\n",
        "\n",
        "    total_loss /= (batch_idx + 1)\n",
        "    return total_loss, metrics\n",
        "\n",
        "\n",
        "def test_epoch(val_loader, model, loss_fn, cuda, metrics):\n",
        "    with torch.no_grad():\n",
        "        for metric in metrics:\n",
        "            metric.reset()\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            target = target.long() if len(target) > 0 else None\n",
        "            if not type(data) in (tuple, list):\n",
        "                data = (data,)\n",
        "            if cuda:\n",
        "                data = tuple(d.cuda() for d in data)\n",
        "                if target is not None:\n",
        "                    target = target.cuda()\n",
        "\n",
        "            outputs = model(*data)\n",
        "\n",
        "            if type(outputs) not in (tuple, list):\n",
        "                outputs = (outputs,)\n",
        "            loss_inputs = outputs\n",
        "            if target is not None:\n",
        "                target = (target,)\n",
        "                loss_inputs += target\n",
        "\n",
        "            loss_outputs = loss_fn(*loss_inputs)\n",
        "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            for metric in metrics:\n",
        "                metric(outputs, target, loss_outputs)\n",
        "\n",
        "    return val_loss, metrics\n",
        "\n",
        "\n",
        "# Triplet generation function\n",
        "class Triplet(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, train):\n",
        "        self.dataset =  dataset\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.train_labels = self.dataset[:,0]\n",
        "            self.train_data = self.dataset[:,1:]\n",
        "            self.labels_set = set(self.train_labels)\n",
        "            self.label_to_indices = {label: np.where(self.train_labels == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "        else:\n",
        "            self.test_labels = self.dataset[:,0]\n",
        "            self.test_data = self.dataset[:,1:]\n",
        "            # generate fixed triplets for testing\n",
        "            self.labels_set = set(self.test_labels)\n",
        "            self.label_to_indices = {label: np.where(self.test_labels == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "            random_state = np.random.RandomState(29)\n",
        "\n",
        "            triplets = [[i,\n",
        "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
        "                         random_state.choice(self.label_to_indices[\n",
        "                                                 np.random.choice(\n",
        "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
        "                                                 )\n",
        "                                             ])\n",
        "                         ]\n",
        "                        for i in range(len(self.test_data))]\n",
        "            self.test_triplets = triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            sample1, label1 = self.train_data[index], self.train_labels[index].item()\n",
        "            positive_index = index\n",
        "            while positive_index == index:\n",
        "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
        "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
        "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
        "            sample2 = self.train_data[positive_index]\n",
        "            sample3 = self.train_data[negative_index]\n",
        "        else:\n",
        "            sample1 = self.test_data[self.test_triplets[index][0]]\n",
        "            sample2 = self.test_data[self.test_triplets[index][1]]\n",
        "            sample3 = self.test_data[self.test_triplets[index][2]]\n",
        "\n",
        "        return (sample1, sample2, sample3), []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEl73cHotMfU"
      },
      "source": [
        "## MODELS USED FOR AMIKACIN\n",
        "\n",
        "# Backbone network used for both Siamese network and MLP\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        self.fc = nn.Sequential(nn.Linear(284449, 512),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Dropout(p = 0.4),\n",
        "                                nn.Linear(512, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Dropout(p = 0.4),\n",
        "                                nn.Linear(256, 64),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(64, 16)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        output = self.fc(x)\n",
        "        output = output.float()\n",
        "        return output\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "# Siamese Network\n",
        "class TripletNet(nn.Module):\n",
        "    def __init__(self, embedding_net):\n",
        "      \n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        output1 = self.embedding_net(x1).float()\n",
        "        output2 = self.embedding_net(x2).float()\n",
        "        output3 = self.embedding_net(x3).float()\n",
        "        return output1, output2, output3\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.embedding_net(x)\n",
        "    \n",
        "# MLP\n",
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self, embedding_net, n_classes):\n",
        "        super(ClassificationNet, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "        self.n_classes = n_classes\n",
        "        self.nonlinear = nn.PReLU()\n",
        "        self.fc1 = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding_net(x)\n",
        "        output = self.nonlinear(output)\n",
        "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        output = self.embedding_net(x)\n",
        "        output = self.nonlinear(output)\n",
        "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFA_reH_yH8q"
      },
      "source": [
        "## MODELS USED FOR OFLOXACIN\n",
        "\n",
        "# Backbone network used for both Siamese network and MLP\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        self.fc = nn.Sequential(nn.Linear(103954, 2048),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Dropout(p = 0.4),\n",
        "                                nn.Linear(2048, 1024),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Dropout(p = 0.4),\n",
        "                                nn.Linear(1024, 512),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Dropout(p = 0.4),\n",
        "                                nn.Linear(512, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 16)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        output = self.fc(x)\n",
        "        output = output.float()\n",
        "        return output\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "# Siamese Network\n",
        "class TripletNet(nn.Module):\n",
        "    def __init__(self, embedding_net):\n",
        "      \n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        output1 = self.embedding_net(x1).float()\n",
        "        output2 = self.embedding_net(x2).float()\n",
        "        output3 = self.embedding_net(x3).float()\n",
        "        return output1, output2, output3\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.embedding_net(x)\n",
        "\n",
        "# MLP\n",
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self, embedding_net, n_classes):\n",
        "        super(ClassificationNet, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "        self.n_classes = n_classes\n",
        "        self.nonlinear = nn.PReLU()\n",
        "        self.fc1 = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding_net(x)\n",
        "        output = self.nonlinear(output)\n",
        "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        output = self.embedding_net(x)\n",
        "        output = self.nonlinear(output)\n",
        "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
        "        return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLiBlngnfLC-"
      },
      "source": [
        "## MODELS USED FOR ETHIONAMIDE\n",
        "\n",
        "# Backbone network used for both Siamese network and MLP\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(1503, 512),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Dropout(p = 0.4),\n",
        "                                nn.Linear(512, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 16)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        output = self.fc(x)\n",
        "        output = output.float()\n",
        "        return output\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "# Siamese Network\n",
        "class TripletNet(nn.Module):\n",
        "    def __init__(self, embedding_net):\n",
        "      \n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        output1 = self.embedding_net(x1).float()\n",
        "        output2 = self.embedding_net(x2).float()\n",
        "        output3 = self.embedding_net(x3).float()\n",
        "        return output1, output2, output3\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.embedding_net(x)\n",
        "\n",
        "# MLP\n",
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self, embedding_net, n_classes):\n",
        "        super(ClassificationNet, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "        self.n_classes = n_classes\n",
        "        self.nonlinear = nn.PReLU()\n",
        "        self.fc1 = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding_net(x)\n",
        "        output = self.nonlinear(output)\n",
        "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        output = self.embedding_net(x)\n",
        "        output = self.nonlinear(output)\n",
        "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KadAC9TZzju"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ0AbfOUZz9Q"
      },
      "source": [
        "Training MLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0gOgpPSrAQx"
      },
      "source": [
        "path = \"/content/drive/MyDrive/bacteria data/ofloxacin/\" #Folder containing X and Y (X is the feature matrix and Y has the target labels)\n",
        "X = np.load(path+\"X.npy\")\n",
        "Y = np.load(path+\"Y.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrmNg0w6W6Ry"
      },
      "source": [
        "Y = (Y+1)/2 #the labels are saved as -1 and 1, this line converts them to 0 and 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZejeduoKrV40"
      },
      "source": [
        "data = np.c_[Y,X]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGu151LTslFs"
      },
      "source": [
        "np.random.shuffle(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZbvRph1tLTA"
      },
      "source": [
        "n_samples = data.shape[0]\n",
        "num_train = round(n_samples * 0.6)\n",
        "num_val = round(n_samples * 0.2)\n",
        "train_dataset = data[:num_train]\n",
        "val_dataset = data[num_train:num_train+num_val]\n",
        "test_dataset = data[num_val+num_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6nbbdo6t9HX"
      },
      "source": [
        "print(train_dataset.shape, val_dataset.shape, test_dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKALRKC_RLPg"
      },
      "source": [
        "mlp_train_dataset = []\n",
        "for i in train_dataset:\n",
        "  mlp_train_dataset.append((i[1:],i[0]))\n",
        "\n",
        "mlp_val_dataset = []\n",
        "for i in val_dataset:\n",
        "  mlp_val_dataset.append((i[1:],i[0]))\n",
        "\n",
        "mlp_test_dataset = []\n",
        "for i in test_dataset:\n",
        "  mlp_test_dataset.append((i[1:],i[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ6r_XwNRLR-"
      },
      "source": [
        "batch_size = 700\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(mlp_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(mlp_val_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(mlp_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "# Set up the network and training parameters\n",
        "\n",
        "\n",
        "embedding_net = EmbeddingNet()\n",
        "model = ClassificationNet(embedding_net, n_classes=2)\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 100\n",
        "log_interval = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkPg0L5QRLVC"
      },
      "source": [
        "fit(train_loader, test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCTMaMQnZ2d0"
      },
      "source": [
        "classes = ['Susceptible', 'Resistant']\n",
        "class_no = [0,1]\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "\n",
        "# Can be used to plot the first 2 dimensions of the embeddings\n",
        "def plot_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(10):\n",
        "        inds = np.where(targets==i)[0]\n",
        "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
        "    if xlim:\n",
        "        plt.xlim(xlim[0], xlim[1])\n",
        "    if ylim:\n",
        "        plt.ylim(ylim[0], ylim[1])\n",
        "    plt.legend(classes)\n",
        "\n",
        "def extract_embeddings(dataloader, model):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = np.zeros((len(dataloader.dataset), 2))\n",
        "        labels = np.zeros(len(dataloader.dataset))\n",
        "        k = 0\n",
        "        for images, target in dataloader:\n",
        "            if cuda:\n",
        "                images = images.cuda()\n",
        "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
        "            labels[k:k+len(images)] = target.numpy()\n",
        "            k += len(images)\n",
        "    return embeddings, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PCuwt28ZTfZ"
      },
      "source": [
        "train_embeddings_tl, train_labels_tl = extract_embeddings(train_loader, model)\n",
        "val_embeddings_tl, val_labels_tl = extract_embeddings(val_loader, model)\n",
        "test_embeddings_tl, test_labels_tl = extract_embeddings(test_loader, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQfIiXdxZXW9"
      },
      "source": [
        "# Obtain the validation and test metrics (auroc, auprc, f1 score)\n",
        "\n",
        "val_y = np.argmax(val_embeddings_tl, axis = 1)\n",
        "val_auroc = roc_auc_score(val_labels_tl, val_embeddings_tl[:, 1])\n",
        "precision, recall, threshold = precision_recall_curve(val_labels_tl, val_embeddings_tl[:, 1])\n",
        "val_auprc = auc(recall, precision)\n",
        "val_f1 = f1_score(val_labels_tl, val_y)\n",
        "\n",
        "test_y = np.argmax(test_embeddings_tl, axis = 1)\n",
        "test_auroc = roc_auc_score(test_labels_tl, test_embeddings_tl[:,1])\n",
        "precision, recall, threshold = precision_recall_curve(test_labels_tl, test_embeddings_tl[:,1])\n",
        "test_auprc = auc(recall, precision)\n",
        "test_f1 = f1_score(test_labels_tl, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_sfP5owahJd"
      },
      "source": [
        "print(\"Validation AUROC -\", val_auroc)\n",
        "print(\"Validation AUPRC -\", val_auprc)\n",
        "print(\"Validation F1 Score -\", val_f1)\n",
        "\n",
        "print(\"Test AUROC -\", test_auroc)\n",
        "print(\"Test AUPRC -\", test_auprc)\n",
        "print(\"Test F1 Score -\", test_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDsV3XUWbTGw"
      },
      "source": [
        "Training Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEYjEIbecpbk"
      },
      "source": [
        "path = \"/content/drive/MyDrive/bacteria data/ofloxacin/\" #Folder containing X and Y (X is the feature matrix and Y has the target labels)\n",
        "X = np.load(path+\"X.npy\")\n",
        "Y = np.load(path+\"Y.npy\")\n",
        "\n",
        "data = np.c_[Y,X]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prwZa2iScu1b"
      },
      "source": [
        "np.random.shuffle(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozO_yYKpcy2Q"
      },
      "source": [
        "n_samples = data.shape[0]\n",
        "num_train = round(n_samples * 0.6)\n",
        "num_val = round(n_samples * 0.2)\n",
        "train_dataset = data[:num_train]\n",
        "val_dataset = data[num_train:num_train+num_val]\n",
        "test_dataset = data[num_val+num_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d14-KZzHc378"
      },
      "source": [
        "print(train_dataset.shape, val_dataset.shape, test_dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-KgC36QqYW9"
      },
      "source": [
        "triplet_train_dataset = Triplet(train_dataset, True) # Returns triplets of train dataset\n",
        "triplet_val_dataset = Triplet(val_dataset, False) # Returns triplets of validation dataset\n",
        "batch_size = 700\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "triplet_val_loader = torch.utils.data.DataLoader(triplet_val_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "# Set up the network and training parameters\n",
        "\n",
        "## HYPERPARAMETERS\n",
        "margin = 1.\n",
        "embedding_net = EmbeddingNet()\n",
        "model = TripletNet(embedding_net)\n",
        "model = model.float()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "loss_fn = TripletLoss(margin)\n",
        "lr = 0.005\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 100\n",
        "log_interval = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NusuKx2Rxkqy"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoCgIgcjq_L6"
      },
      "source": [
        "fit(triplet_train_loader, triplet_val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr449AdWuTVx"
      },
      "source": [
        "classes = ['Susceptible', 'Resistant']\n",
        "class_no = [-1,1]\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# Can be used to plot the first 2 dimensions of the embeddings\n",
        "def plot_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(2):\n",
        "        inds = np.where(targets==class_no[i])[0]\n",
        "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
        "    if xlim:\n",
        "        plt.xlim(xlim[0], xlim[1])\n",
        "    if ylim:\n",
        "        plt.ylim(ylim[0], ylim[1])\n",
        "    plt.legend(classes)\n",
        "\n",
        "def extract_embeddings(dataloader, model):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = np.zeros((len(dataloader.dataset), 16))\n",
        "        labels = np.zeros(len(dataloader.dataset))\n",
        "        k = 0\n",
        "        for i in dataloader:\n",
        "            \n",
        "            target = i[:, 0]\n",
        "            images = i[:, 1:]\n",
        "            if cuda:\n",
        "                images = images.cuda()\n",
        "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
        "            labels[k:k+len(images)] = target.numpy()\n",
        "            k += len(images)\n",
        "    return embeddings, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh-O6igexQ1N"
      },
      "source": [
        "train_embeddings_tl, train_labels_tl = extract_embeddings(train_loader, model)\n",
        "val_embeddings_tl, val_labels_tl = extract_embeddings(val_loader, model)\n",
        "test_embeddings_tl, test_labels_tl = extract_embeddings(test_loader, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znCs3PEIx-tX"
      },
      "source": [
        "# KNN algorithm to perform classification on the siamese network embeddings\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=15, weights = 'distance', algorithm='auto')\n",
        "neigh.fit(train_embeddings_tl, train_labels_tl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99EBp-WZI-R0"
      },
      "source": [
        "val_proba = neigh.predict_proba(val_embeddings_tl)\n",
        "test_proba = neigh.predict_proba(test_embeddings_tl)\n",
        "train_proba = neigh.predict_proba(train_embeddings_tl)\n",
        "val_y = neigh.predict(val_embeddings_tl)\n",
        "test_y = neigh.predict(test_embeddings_tl)\n",
        "train_y = neigh.predict(train_embeddings_tl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlkRb3vdKC8w"
      },
      "source": [
        "# Obtain the validation and test metrics (auroc, auprc, f1 score)\n",
        "\n",
        "val_auroc = roc_auc_score(val_labels_tl, val_proba[:, 1])\n",
        "precision, recall, threshold = precision_recall_curve(val_labels_tl, val_proba[:, 1])\n",
        "val_auprc = auc(recall, precision)\n",
        "val_f1 = f1_score(val_labels_tl, val_y)\n",
        "\n",
        "test_auroc = roc_auc_score(test_labels_tl, test_proba[:, 1])\n",
        "precision, recall, threshold = precision_recall_curve(test_labels_tl, test_proba[:, 1])\n",
        "test_auprc = auc(recall, precision)\n",
        "test_f1 = f1_score(test_labels_tl, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLvgsWSNK9Ag"
      },
      "source": [
        "print(\"Validation AUROC -\", val_auroc)\n",
        "print(\"Validation AUPRC -\", val_auprc)\n",
        "print(\"Validation F1 Score -\", val_f1)\n",
        "\n",
        "print(\"Test AUROC -\", test_auroc)\n",
        "print(\"Test AUPRC -\", test_auprc)\n",
        "print(\"Test F1 Score -\", test_f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}